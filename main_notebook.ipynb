{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content    \n",
    "!git clone https://github.com/Giovannicus/GourmetAI.git     \n",
    "%cd GourmetAI   \n",
    "!git checkout colab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzUCo5fJdbO4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcusumano\\ProfAI\\GourmetAI\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning:\n",
      "\n",
      "A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "def download_and_unzip(url, extract_to=\".\"):\n",
    "\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    zip_path = os.path.join(extract_to, 'dataset.zip')\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(zip_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=128):\n",
    "            file.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    os.remove(zip_path)\n",
    "    print(f\"I file sono stati estratti in: {extract_to}\")\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou1scoZfbltP",
    "outputId": "c8069c8a-fa99-4028-8cda-9b57e22bb47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I file sono stati estratti in: C:\\Users\\gcusumano\\ProfAI\\GourmetAI\n"
     ]
    }
   ],
   "source": [
    "download_and_unzip(\"https://proai-datasets.s3.eu-west-3.amazonaws.com/dataset_food_classification.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "nJ2nqRlmdSGe"
   },
   "outputs": [],
   "source": [
    "class Transforms:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transforms(image=np.array(img))['image']\n",
    "\n",
    "transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.HorizontalFlip(),\n",
    "        A.Rotate(limit=10),\n",
    "        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "      ])\n",
    "\n",
    "transform2 = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "wQzscBNVhTF1"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root='dataset/train', transform=Transforms(transform))\n",
    "\n",
    "valset = torchvision.datasets.ImageFolder(root='dataset/val', transform=Transforms(transform))\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='dataset/test',transform=Transforms(transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGH1S9XphWP-",
    "outputId": "b23e77fa-970e-437d-df8d-768e392fe343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8960, 2240, 2800)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "tmRwbkfZhd6_"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_batches = 15\n",
    "num_samples = num_batches * batch_size\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Funzione per ottenere indici bilanciati\n",
    "def get_balanced_indices(dataset, num_samples):\n",
    "    # Raggruppiamo gli indici per classe\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Calcoliamo quanti campioni per classe vogliamo\n",
    "    num_classes = len(class_indices)\n",
    "    samples_per_class = num_samples // num_classes\n",
    "    \n",
    "    # Selezioniamo in modo bilanciato\n",
    "    balanced_indices = []\n",
    "    for class_idx in class_indices:\n",
    "        indices = class_indices[class_idx]\n",
    "        random.shuffle(indices)\n",
    "        balanced_indices.extend(indices[:samples_per_class])\n",
    "    \n",
    "    # Shuffle finale degli indici bilanciati\n",
    "    random.shuffle(balanced_indices)\n",
    "    return balanced_indices\n",
    "\n",
    "# Otteniamo indici bilanciati per train e val\n",
    "train_indices = get_balanced_indices(trainset, len(trainset))\n",
    "val_indices = get_balanced_indices(valset, len(valset))\n",
    "test_indices = get_balanced_indices(testset, len(testset))\n",
    "\n",
    "limited_trainset = Subset(trainset, train_indices)\n",
    "limited_valset = Subset(valset, val_indices)\n",
    "limited_testset = Subset(testset, test_indices)\n",
    "# Creiamo i dataloader\n",
    "trainloader = torch.utils.data.DataLoader(limited_trainset, \n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)  # Manteniamo anche lo shuffle nel DataLoader\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(limited_valset, \n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(limited_testset, \n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, \n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle=True,     # Shuffle per il training\n",
    "#                                         num_workers=4)    # Aggiungo worker per velocizzare il caricamento\n",
    "\n",
    "# valloader = torch.utils.data.DataLoader(valset, \n",
    "#                                       batch_size=batch_size,\n",
    "#                                       shuffle=False,      # No shuffle per validation\n",
    "#                                       num_workers=4)\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(testset, \n",
    "#                                        batch_size=batch_size,\n",
    "#                                        shuffle=False,     # No shuffle per test\n",
    "#                                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: tensor([36., 36., 36., 36., 36., 36., 36., 36., 36., 36., 36., 36., 36., 36.])\n"
     ]
    }
   ],
   "source": [
    "label_counts = torch.zeros(14)  # Assuming `num_classes` is known\n",
    "for _, labels in trainloader:\n",
    "    for label in labels:\n",
    "        label_counts[label] += 1\n",
    "print(\"Label distribution:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9b_GjsxhkaY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "           Linear-33                   [-1, 14]           7,182\n",
      "================================================================\n",
      "Total params: 14,721,870\n",
      "Trainable params: 7,182\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.40\n",
      "Params size (MB): 56.16\n",
      "Estimated Total Size (MB): 275.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vgg = vgg16(weights = VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "class VGGClassifier(nn.Module):\n",
    "    def __init__(self, vgg_backbone, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = vgg_backbone.features\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x) \n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "vgg = VGGClassifier(vgg,512,14).to(device)\n",
    "summary(vgg,(3, 224, 224), device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logger + exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "def get_logger(ch_log_level=logging.ERROR, \n",
    "               fh_log_level=logging.INFO):\n",
    "    logging.shutdown()\n",
    "    importlib.reload(logging)  # Sostituito imp.reload con importlib.reload\n",
    "    logger = logging.getLogger(\"cheatsheet\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Console Handler\n",
    "    if ch_log_level:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(ch_log_level)\n",
    "        ch.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(ch)\n",
    "    \n",
    "    # File Handler\n",
    "    if fh_log_level:\n",
    "        fh = logging.FileHandler('cheatsheet.log')\n",
    "        fh.setLevel(fh_log_level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class Experiment():\n",
    "    def __init__(self, name, root, logger=None):\n",
    "        self.name = name\n",
    "        self.root = os.path.join(root, name)\n",
    "        self.logger = logger\n",
    "        self.epoch = 1\n",
    "        self.best_val_loss = sys.maxsize\n",
    "        self.best_val_loss_epoch = 1\n",
    "        self.weights_dir = os.path.join(self.root, 'weights')\n",
    "        self.history_dir = os.path.join(self.root, 'history')\n",
    "        self.results_dir = os.path.join(self.root, 'results')\n",
    "        self.latest_weights = os.path.join(self.weights_dir, 'latest_weights.pth')\n",
    "        self.latest_optimizer = os.path.join(self.weights_dir, 'latest_optim.pth')\n",
    "        self.best_weights_path = self.latest_weights\n",
    "        self.best_optimizer_path = self.latest_optimizer\n",
    "        self.train_history_fpath = os.path.join(self.history_dir, 'train.csv')\n",
    "        self.val_history_fpath = os.path.join(self.history_dir, 'val.csv')\n",
    "        self.test_history_fpath = os.path.join(self.history_dir, 'test.csv')\n",
    "        self.loss_history = {\n",
    "            'train': np.array([]),\n",
    "            'val': np.array([]),\n",
    "            'test': np.array([])\n",
    "        }\n",
    "        self.precision_history = { \n",
    "            'train': np.array([]),\n",
    "            'val': np.array([]),\n",
    "            'test': np.array([])\n",
    "        }\n",
    "        \n",
    "    def log(self, msg):\n",
    "        if self.logger:\n",
    "            self.logger.info(msg)\n",
    "        \n",
    "    def init(self):\n",
    "        self.log(\"Creating new experiment\")\n",
    "        self.init_dirs()\n",
    "        self.init_history_files()\n",
    "\n",
    "    def resume(self, model, optim, weights_fpath=None, optim_path=None):\n",
    "        self.log(\"Resuming existing experiment\")\n",
    "        if weights_fpath is None:\n",
    "            weights_fpath = self.latest_weights\n",
    "        if optim_path is None:\n",
    "            optim_path = self.latest_optimizer\n",
    "\n",
    "        model, state = self.load_weights(model, weights_fpath)\n",
    "        optim = self.load_optimizer(optim, optim_path)\n",
    "\n",
    "        self.best_val_loss = state['best_val_loss']\n",
    "        self.best_val_loss_epoch = state['best_val_loss_epoch']\n",
    "        self.epoch = state['last_epoch'] + 1\n",
    "        self.load_history_from_file('train')\n",
    "        self.load_history_from_file('val')\n",
    "\n",
    "        return model, optim\n",
    "\n",
    "    def init_dirs(self):\n",
    "        os.makedirs(self.weights_dir, exist_ok=True)\n",
    "        os.makedirs(self.history_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        \n",
    "    def init_history_files(self):\n",
    "        Path(self.train_history_fpath).touch()\n",
    "        Path(self.val_history_fpath).touch()\n",
    "        Path(self.test_history_fpath).touch()\n",
    "\n",
    "   \n",
    "    def load_history_from_file(self, dset_type):\n",
    "        fpath = os.path.join(self.history_dir, dset_type + '.csv')\n",
    "        try:\n",
    "            data = np.loadtxt(fpath, delimiter=',').reshape(-1, 3)\n",
    "            self.loss_history[dset_type] = data[:, 1]\n",
    "            self.precision_history[dset_type] = data[:, 2] \n",
    "        except:\n",
    "            self.loss_history[dset_type] = np.array([])\n",
    "            self.precision_history[dset_type] = np.array([])  \n",
    "\n",
    "\n",
    "    def append_history_to_file(self, dset_type, loss, precision):\n",
    "        fpath = os.path.join(self.history_dir, dset_type + '.csv')\n",
    "        with open(fpath, 'a') as f:\n",
    "            f.write('{},{},{}\\n'.format(self.epoch, loss, precision))\n",
    "\n",
    "    def save_history(self, dset_type, loss, precision): \n",
    "        self.loss_history[dset_type] = np.append(self.loss_history[dset_type], loss)\n",
    "        self.precision_history[dset_type] = np.append(self.precision_history[dset_type], precision)  \n",
    "        self.append_history_to_file(dset_type, loss, precision)  \n",
    "\n",
    "        if dset_type == 'val' and self.is_best_loss(loss):\n",
    "            self.best_val_loss = loss\n",
    "            self.best_val_loss_epoch = self.epoch\n",
    "            \n",
    "        if dset_type == 'val':\n",
    "            self.plot_and_save_history()\n",
    "\n",
    "    def is_best_loss(self, loss):\n",
    "        return loss < self.best_val_loss\n",
    "\n",
    "    def save_weights(self, model, trn_loss, val_loss, trn_precision, val_precision):\n",
    "        weights_fname = self.name + '-weights-%d-%.3f-%.3f-%.3f-%.3f.pth' % (\n",
    "            self.epoch, trn_loss, trn_precision, val_loss, val_precision)\n",
    "        weights_fpath = os.path.join(self.weights_dir, weights_fname)\n",
    "        torch.save({\n",
    "            'last_epoch': self.epoch,\n",
    "            'trn_loss': trn_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'trn_precision': trn_precision,\n",
    "            'val_precision': val_precision,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_val_loss_epoch': self.best_val_loss_epoch,\n",
    "            'experiment': self.name,\n",
    "            'state_dict': model.state_dict()\n",
    "        }, weights_fpath)\n",
    "        shutil.copyfile(weights_fpath, self.latest_weights)\n",
    "        if self.is_best_loss(val_loss):\n",
    "            self.best_weights_path = weights_fpath\n",
    "\n",
    "    def load_weights(self, model, fpath):\n",
    "        self.log(\"loading weights '{}'\".format(fpath))\n",
    "        state = torch.load(fpath,map_location=torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        self.log(\"loaded weights from experiment %s (last_epoch %d, trn_loss %s, trn_precision %s, val_loss %s, val_precision %s)\" % (\n",
    "            self.name, state['last_epoch'], state['trn_loss'],\n",
    "            state['trn_precision'], state['val_loss'], state['val_precision']))\n",
    "        return model, state\n",
    "\n",
    "    def save_optimizer(self, optimizer, val_loss):\n",
    "        optim_fname = self.name + '-optim-%d.pth' % (self.epoch)\n",
    "        optim_fpath = os.path.join(self.weights_dir, optim_fname)\n",
    "        torch.save({\n",
    "            'last_epoch': self.epoch,\n",
    "            'experiment': self.name,\n",
    "            'state_dict': optimizer.state_dict()\n",
    "        }, optim_fpath)\n",
    "        shutil.copyfile(optim_fpath, self.latest_optimizer)\n",
    "        if self.is_best_loss(val_loss):\n",
    "            self.best_optimizer_path = optim_fpath\n",
    "\n",
    "    def load_optimizer(self, optimizer, fpath):\n",
    "        self.log(\"loading optimizer '{}'\".format(fpath))\n",
    "        optim = torch.load(fpath,map_location=torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        optimizer.load_state_dict(optim['state_dict'])\n",
    "        self.log(\"loaded optimizer from session {}, last_epoch {}\"\n",
    "                 .format(optim['experiment'], optim['last_epoch']))\n",
    "        return optimizer\n",
    "\n",
    "    def plot_and_save_history(self):\n",
    "        if not hasattr(self, 'monitor'):\n",
    "            self.monitor = ColabTrainingMonitor(figsize=(12, 8))\n",
    "            \n",
    "        try:\n",
    "            if os.path.exists(self.train_history_fpath) and os.path.exists(self.val_history_fpath):\n",
    "                train_data = np.loadtxt(self.train_history_fpath, delimiter=',')\n",
    "                val_data = np.loadtxt(self.val_history_fpath, delimiter=',')\n",
    "                \n",
    "                if train_data.ndim == 1:\n",
    "                    train_data = train_data.reshape(1, -1)\n",
    "                if val_data.ndim == 1:\n",
    "                    val_data = val_data.reshape(1, -1)\n",
    "                \n",
    "                latest_epoch = int(train_data[-1, 0])\n",
    "                train_loss = train_data[-1, 1]\n",
    "                train_precision = train_data[-1, 2]  \n",
    "                val_loss = val_data[-1, 1]\n",
    "                val_precision = val_data[-1, 2]  \n",
    "                \n",
    "                self.monitor.update(\n",
    "                    epoch=latest_epoch,\n",
    "                    train_loss=train_loss,\n",
    "                    val_loss=val_loss,\n",
    "                    train_precision=train_precision, \n",
    "                    val_precision=val_precision     \n",
    "                )\n",
    "                \n",
    "                self.monitor.save(os.path.join(self.history_dir, 'training_progress.png'))\n",
    "                \n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.warning(f\"Error updating plot: {e}\")\n",
    "\n",
    "    # def plot_and_save_history(self):\n",
    "    #     if not hasattr(self, 'interactive_plot'):\n",
    "    #         self.interactive_plot = InteractivePlot(\n",
    "    #             self.train_history_fpath, \n",
    "    #             self.val_history_fpath, \n",
    "    #             self.history_dir\n",
    "    #         )\n",
    "    #         # Avvia il plotting in un thread separato\n",
    "    #         from threading import Thread\n",
    "    #         self.plot_thread = Thread(target=self.interactive_plot.run, args=(1,), daemon=True)\n",
    "    #         self.plot_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### static interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class SimpleTrainingMonitor:\n",
    "    def __init__(self, train_history_fpath, val_history_fpath):\n",
    "        self.train_history_fpath = train_history_fpath\n",
    "        self.val_history_fpath = val_history_fpath\n",
    "    \n",
    "    def create_figure(self):\n",
    "        \"\"\"Create and return the plotly figure\"\"\"\n",
    "        # Leggi i dati\n",
    "        train_df = pd.read_csv(self.train_history_fpath, names=['epoch', 'loss', 'precision'])\n",
    "        val_df = pd.read_csv(self.val_history_fpath, names=['epoch', 'loss', 'precision'])\n",
    "        \n",
    "        # Crea subplot\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=('Loss Over Time', 'Precision Over Time'),\n",
    "            vertical_spacing=0.15\n",
    "        )\n",
    "        \n",
    "        # Aggiungi tracce per il training\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=train_df['epoch'], y=train_df['loss'], \n",
    "                      name='Train Loss', mode='lines+markers',\n",
    "                      line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=train_df['epoch'], y=train_df['precision'],\n",
    "                      name='Train Precision', mode='lines+markers',\n",
    "                      line=dict(color='blue')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Aggiungi tracce per la validation\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=val_df['epoch'], y=val_df['loss'],\n",
    "                      name='Val Loss', mode='lines+markers',\n",
    "                      line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=val_df['epoch'], y=val_df['precision'],\n",
    "                      name='Val Precision', mode='lines+markers',\n",
    "                      line=dict(color='red')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Aggiorna il layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            title_text=\"Training Progress\",\n",
    "            title_x=0.5,\n",
    "            plot_bgcolor='white',  # sfondo bianco\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgrey',\n",
    "            gridwidth=0.2,\n",
    "            griddash='dot',  # oppure usa 'dot' per punti invece che trattini\n",
    "            # oppure usa un pattern personalizzato:\n",
    "            # griddash='2,2',  # numeri più piccoli = trattini più corti e più vicini\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            title_text=\"Epoch\"\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgrey',\n",
    "            gridwidth=0.2,\n",
    "            griddash='dot',  # oppure 'dot' per punti\n",
    "            # oppure griddash='2,2',\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            linewidth=1\n",
    "        )\n",
    "        \n",
    "        # Imposta specificamente gli assi y\n",
    "        fig.update_yaxes(title_text=\"Loss\", type=\"log\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Precision\", row=2, col=1)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Display the plot\"\"\"\n",
    "        fig = self.create_figure()\n",
    "        return fig  # Questo permetterà a Jupyter di mostrare il plot\n",
    "\n",
    "def plot_training_history(experiment_dir):\n",
    "    \"\"\"Helper function to quickly plot training history\"\"\"\n",
    "    train_path = os.path.join(experiment_dir, \"history\", \"train.csv\")\n",
    "    val_path = os.path.join(experiment_dir, \"history\", \"val.csv\")\n",
    "    \n",
    "    monitor = SimpleTrainingMonitor(train_path, val_path)\n",
    "    return monitor.plot()  # Ritorna la figura invece di mostrarla direttamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colab interaactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor per Colab\n",
    "class ColabTrainingMonitor:\n",
    "    def __init__(self, figsize=(12, 8)):\n",
    "        self.figsize = figsize\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_precisions = []\n",
    "        self.val_precisions = []\n",
    "        self.epochs = []\n",
    "        \n",
    "        # Setup plot style\n",
    "        plt.style.use('default')\n",
    "        \n",
    "    def update(self, epoch, train_loss, val_loss, train_precision, val_precision):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_precisions.append(train_precision)\n",
    "        self.val_precisions.append(val_precision)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        self.plot()\n",
    "        \n",
    "    def plot(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=self.figsize)\n",
    "        fig.suptitle('Training Progress', fontsize=16, y=1.02)\n",
    "        \n",
    "        # Plot Loss\n",
    "        ax1.plot(self.epochs, self.train_losses, 'b-o', label='Training Loss', \n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax1.plot(self.epochs, self.val_losses, 'r-o', label='Validation Loss', \n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax1.set_title('Loss Over Time', pad=10)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax1.legend(loc='upper right', frameon=True)\n",
    "        \n",
    "        # Annotate last values\n",
    "        if self.train_losses:\n",
    "            ax1.annotate(f'{self.train_losses[-1]:.4f}', \n",
    "                        (self.epochs[-1], self.train_losses[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "            ax1.annotate(f'{self.val_losses[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.val_losses[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        \n",
    "        # Plot Precision\n",
    "        ax2.plot(self.epochs, self.train_precisions, 'b-o', label='Training Precision', \n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax2.plot(self.epochs, self.val_precisions, 'r-o', label='Validation Precision', \n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax2.set_title('Precision Over Time', pad=10)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax2.legend(loc='lower right', frameon=True)\n",
    "        ax2.set_ylim(0, 1)  # precision è sempre tra 0 e 1\n",
    "        \n",
    "        # Annotate last values\n",
    "        if self.train_precisions:\n",
    "            ax2.annotate(f'{self.train_precisions[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.train_precisions[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "            ax2.annotate(f'{self.val_precisions[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.val_precisions[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the current plot to a file\"\"\"\n",
    "        self.plot()\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    def get_current_metrics(self):\n",
    "        if not self.epochs:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'epoch': self.epochs[-1],\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'train_precision': self.train_precisions[-1],\n",
    "            'val_precision': self.val_precisions[-1]\n",
    "        }\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Cleanup method\"\"\"\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "\n",
    "def train(net, dataloader, criterion, optimizer, epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net.train()\n",
    "    n_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    total_precision = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ## Forward Pass\n",
    "        output = net(inputs)\n",
    "        \n",
    "        ## Clear Gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "    \n",
    "        ## Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Get predictions and compute precision\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        precision = get_precision(preds, targets)\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        total_precision += precision\n",
    "    \n",
    "    mean_loss = total_loss / n_batches\n",
    "    mean_precision = total_precision / n_batches\n",
    "    return mean_loss, mean_precision\n",
    "\n",
    "def test(net, test_loader, criterion, epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_precision = 0\n",
    "    n_batches = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            batch_precision = get_precision(preds, target)\n",
    "            test_precision += batch_precision\n",
    "            \n",
    "    test_loss /= n_batches\n",
    "    test_precision /= n_batches\n",
    "    \n",
    "    print(f\"\\nFinal validation metrics:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    return test_loss, test_precision\n",
    "\n",
    "def get_predictions(model_output):\n",
    "    \"\"\"Ottiene le predizioni dal output del modello\"\"\"\n",
    "    device = model_output.device  # mantieni lo stesso device dell'output\n",
    "    val, idx = torch.max(model_output, dim=1)\n",
    "    return idx  # mantieni sul device originale\n",
    "\n",
    "def get_precision(preds: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "\n",
    "    device = targets.device\n",
    "    preds = preds.to(device)\n",
    "\n",
    "    preds = manual_one_hot(preds,14)\n",
    "    targets = manual_one_hot(targets,14)\n",
    "    # Inizializza liste per le metriche\n",
    "    precisions = []\n",
    "\n",
    "    # Calcola la precisione per ogni classe\n",
    "    num_classes = preds.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        class_preds = preds[:, class_idx]\n",
    "        class_targets = targets[:, class_idx]\n",
    "        \n",
    "        # Calcola true positives e total predictions\n",
    "        true_positives = (class_preds * class_targets).sum()\n",
    "        false_positives = (class_preds * (1 - class_targets)).sum()\n",
    "        total_predicted = true_positives+false_positives\n",
    "        \n",
    "        # Calcola la precisione per questa classe se ci sono predizioni\n",
    "        if total_predicted > 0:\n",
    "            class_precision = true_positives / total_predicted\n",
    "            precisions.append(class_precision)\n",
    "            \n",
    "        else:\n",
    "            class_precision = torch.nan    \n",
    "            precisions.append(class_precision)\n",
    "    \n",
    "    precision_tensor = torch.tensor(precisions)\n",
    "    mean_precision = precision_tensor.nanmean().item()\n",
    "\n",
    "    return mean_precision\n",
    "\n",
    "# Queste funzioni rimangono invariate perché non sono legate alla metrica\n",
    "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
    "    \"\"\"Sets the learning rate to the initially \n",
    "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
    "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_uniform(m.weight) \n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def save_weights(model, weights_dir, epoch):\n",
    "    weights_fname = 'weights-%d.pth' % (epoch)\n",
    "    weights_fpath = os.path.join(weights_dir, weights_fname)\n",
    "    torch.save({'state_dict': model.state_dict()}, weights_fpath)\n",
    "\n",
    "def load_weights(model, fpath):\n",
    "    state = torch.load(fpath)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "\n",
    "def manual_one_hot(tensor, num_classes=None):\n",
    "    if num_classes is None:\n",
    "        num_classes = tensor.max() + 1\n",
    "    one_hot = torch.zeros((tensor.size(0), num_classes),device=device)\n",
    "    one_hot.scatter_(1, tensor.unsqueeze(1), 1)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m EXPERIMENT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mvgg\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger(ch_log_level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, fh_log_level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "MAX_PATIENCE = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_DECAY = 0.99\n",
    "DECAY_LR_EVERY_N_EPOCHS = 2\n",
    "EXPERIMENT_NAME = 'exp1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = vgg.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "logger = get_logger(ch_log_level=logging.INFO, fh_log_level=logging.INFO)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print('  + Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "exp = Experiment(EXPERIMENT_NAME, \"exps\", logger)\n",
    "#shutil.rmtree(\"exps\\exp1\")\n",
    "# Create New Experiment\n",
    "exp.init()\n",
    "monitor = ColabTrainingMonitor()\n",
    "import time\n",
    "\n",
    "for epoch in range(exp.epoch, exp.epoch+N_EPOCHS):\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_precision = train(model, trainloader, criterion, optimizer, epoch)\n",
    "    logger.info('Epoch {:d}: Train - Loss: {:.4f}\\tPrecision: {:.4f}'.format(\n",
    "        epoch, trn_loss, trn_precision))    \n",
    "    time_elapsed = time.time() - since  \n",
    "    logger.info('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    ### Test ###\n",
    "    val_loss, val_precision = test(model, valloader, criterion, epoch)    \n",
    "    logger.info('Val - Loss: {:.4f}, Precision: {:.4f}'.format(\n",
    "        val_loss, val_precision))\n",
    "    time_elapsed = time.time() - since  \n",
    "    logger.info('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    monitor.update(\n",
    "        epoch=epoch,\n",
    "        train_loss=trn_loss,\n",
    "        val_loss=val_loss,\n",
    "        train_precision=trn_precision, \n",
    "        val_precision=val_precision     \n",
    "    )\n",
    "\n",
    "    ### Save Metrics ###\n",
    "    exp.save_history('train', trn_loss, trn_precision)\n",
    "    exp.save_history('val', val_loss, val_precision)\n",
    "    \n",
    "    ### Checkpoint ###    \n",
    "    exp.save_weights(model, trn_loss, val_loss, trn_precision, val_precision)\n",
    "    exp.save_optimizer(optimizer, val_loss)\n",
    "    \n",
    "    ## Early Stopping ##\n",
    "    if (epoch - exp.best_val_loss_epoch) > MAX_PATIENCE:\n",
    "        logger.info(f\"Early Stopping at epoch {epoch} since no better loss found since epoch {exp.best_val_loss_epoch}\")\n",
    "        break\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "    adjust_learning_rate(LEARNING_RATE, LR_DECAY, optimizer, \n",
    "                         epoch, DECAY_LR_EVERY_N_EPOCHS)\n",
    "    \n",
    "    exp.epoch += 1\n",
    "\n",
    "try:\n",
    "    monitor.save(os.path.join(exp.history_dir, 'final_training_progress.png'))\n",
    "    monitor.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"exp1.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")  # Estrae nella directory corrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "xaxis": "x",
         "y": [
          2.0229532735688345,
          1.3494205372674124,
          1.097415392739432,
          0.9669795393943786,
          0.8832692520959037,
          0.8215823403426579,
          0.7822024524211884,
          0.7462329030036926,
          0.7132247269153595,
          0.6919813249792371,
          0.6709802465779441,
          0.6497110681874412,
          0.6358537256717682,
          0.6226951705557959,
          0.6032812540020261,
          0.5953331576926367,
          0.5890365732567651,
          0.5769379773310253,
          0.5677769247974668,
          0.5584123815808978,
          0.5510100960731507,
          0.5466924497059413,
          0.5372813880443573,
          0.5358492800167629,
          0.5225674931492125,
          0.5185524110283171,
          0.5104323140212468,
          0.5083380137171064,
          0.5026400434119361,
          0.4970872108425412
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines+markers",
         "name": "Train Precision",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "xaxis": "x2",
         "y": [
          0.5234681258776358,
          0.7057939086641584,
          0.735838223355157,
          0.7476783743926457,
          0.775221346105848,
          0.7824589814458575,
          0.7901264786720276,
          0.7934940874576568,
          0.80538461293493,
          0.8098329799515861,
          0.8122287579945155,
          0.8178505046027047,
          0.8213189457144056,
          0.8218315277780806,
          0.8290947786399296,
          0.8266719247613634,
          0.8314164876937866,
          0.8343684443405697,
          0.8293601027556828,
          0.8371197879314423,
          0.8389147017683302,
          0.8411034183842795,
          0.843681994506291,
          0.8420234535421643,
          0.851783515725817,
          0.8477509694440024,
          0.8551594487258366,
          0.8532848341124398,
          0.8528815593038287,
          0.8543217190674373
         ],
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Val Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "xaxis": "x",
         "y": [
          1.594865812195672,
          1.241813666290707,
          1.0844997002018824,
          0.9929949012067584,
          0.9308792584472232,
          0.8747847312026553,
          0.8408482604556613,
          0.8183648222022586,
          0.7982479830582937,
          0.7763034866915809,
          0.761910253100925,
          0.7573864029513465,
          0.7389473186598884,
          0.7238157060411241,
          0.7217933237552643,
          0.7033550341924032,
          0.6871284445126852,
          0.6959854861100515,
          0.6914075911045074,
          0.6819814907179939,
          0.6791924834251404,
          0.6698530581262376,
          0.672434839937422,
          0.6575950351026323,
          0.6655533230966992,
          0.6585836874114143,
          0.6618831422593858,
          0.6580426692962646,
          0.6429681148793962,
          0.6513145532872942
         ],
         "yaxis": "y"
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines+markers",
         "name": "Val Precision",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30
         ],
         "xaxis": "x2",
         "y": [
          0.6458376381132338,
          0.6992929213576846,
          0.7176814609103732,
          0.7381439606348673,
          0.7450825174649557,
          0.7578935523827871,
          0.7653312087059021,
          0.7648385961850485,
          0.7740530603461795,
          0.7808139622211456,
          0.7868409752845764,
          0.7770640552043915,
          0.7832246687677171,
          0.7768451439009773,
          0.780700147151947,
          0.7970755894978842,
          0.8027176691426171,
          0.7905768752098083,
          0.7870072027047476,
          0.7991315821806589,
          0.7961920234892104,
          0.7935494515630934,
          0.7942302525043488,
          0.8006013532479604,
          0.7891319427225325,
          0.7884893980291154,
          0.7907672425111135,
          0.7892610960536532,
          0.8076005743609534,
          0.7958064410421584
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Loss Over Time",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Precision Over Time",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.425,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 800,
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training Progress",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "griddash": "dot",
         "gridwidth": 0.2,
         "linecolor": "black",
         "linewidth": 1,
         "showgrid": true,
         "title": {
          "text": "Epoch"
         },
         "zeroline": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "griddash": "dot",
         "gridwidth": 0.2,
         "linecolor": "black",
         "linewidth": 1,
         "showgrid": true,
         "title": {
          "text": "Epoch"
         },
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.575,
          1
         ],
         "gridcolor": "lightgrey",
         "griddash": "dot",
         "gridwidth": 0.2,
         "linecolor": "black",
         "linewidth": 1,
         "showgrid": true,
         "title": {
          "text": "Loss"
         },
         "type": "log",
         "zeroline": false
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          0.425
         ],
         "gridcolor": "lightgrey",
         "griddash": "dot",
         "gridwidth": 0.2,
         "linecolor": "black",
         "linewidth": 1,
         "showgrid": true,
         "title": {
          "text": "Precision"
         },
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_training_history('exps/exp1')\n",
    "fig  # In Jupyter, l'ultima espressione viene visualizzata automaticamente\n",
    "\n",
    "# Metodo 2: usando la classe direttamente\n",
    "monitor = SimpleTrainingMonitor(\n",
    "    train_history_fpath='exps/exp1/history/train.csv',\n",
    "    val_history_fpath='exps/exp1/history/val.csv'\n",
    ")\n",
    "fig = monitor.plot()\n",
    "fig  # Mostra il plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gcusumano\\AppData\\Local\\Temp\\ipykernel_43148\\732116560.py:154: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "C:\\Users\\gcusumano\\AppData\\Local\\Temp\\ipykernel_43148\\732116560.py:175: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(name=\"exp1\", root=\"exps\")\n",
    "\n",
    "# Per caricare i migliori pesi (invece degli ultimi)\n",
    "model, optimizer = experiment.resume(\n",
    "    model=model,  # il tuo modello vuoto\n",
    "    optim=optimizer,  # il tuo ottimizzatore inizializzato\n",
    "    weights_fpath=experiment.best_weights_path,  # usa i migliori pesi invece degli ultimi\n",
    "    optim_path=experiment.best_optimizer_path  # usa il miglior ottimizzatore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(net, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_precision = 0\n",
    "    n_batches = len(test_loader)\n",
    "    \n",
    "    # Inizializza lista per tutte le predizioni e target\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            batch_precision = get_precision(preds, target)\n",
    "            test_precision += batch_precision\n",
    "            \n",
    "            # Raccogli tutte le predizioni e target\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            \n",
    "    test_loss /= n_batches\n",
    "    test_precision /= n_batches\n",
    "    \n",
    "    # Calcola la confusion matrix\n",
    "    confusion_matrix = get_confusion_matrix(\n",
    "        torch.tensor(all_preds), \n",
    "        torch.tensor(all_targets)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFinal validation metrics:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    print(\"\\nMetrics from confusion matrix:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def get_confusion_matrix(preds: torch.Tensor, targets: torch.Tensor):\n",
    "   \"\"\"\n",
    "   Calcola la matrice di confusione dato predictions e targets.\n",
    "   \n",
    "   Args:\n",
    "       preds: torch.Tensor - Predizioni del modello (già processate con argmax se necessario)\n",
    "       targets: torch.Tensor - Target reali\n",
    "       \n",
    "   Returns:\n",
    "       confusion_matrix: torch.Tensor - Matrice di confusione 2x2\n",
    "   \"\"\"\n",
    "   # Assicurati che siano sul CPU e siano float\n",
    "   preds = preds.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   targets = targets.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "   # Inizializza la matrice di confusione\n",
    "   confusion_matrix = torch.zeros(14, 14)\n",
    "   \n",
    "   # Popola la matrice\n",
    "   for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "       confusion_matrix[t.long(), p.long()] += 1\n",
    "       \n",
    "   return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 32\u001b[0m, in \u001b[0;36mfinal_test\u001b[1;34m(net, test_loader, criterion)\u001b[0m\n\u001b[0;32m     29\u001b[0m test_precision \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_batches\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calcola la confusion matrix\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal validation metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[79], line 60\u001b[0m, in \u001b[0;36mget_confusion_matrix\u001b[1;34m(preds, targets)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mCalcola la matrice di confusione dato predictions e targets.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    confusion_matrix: torch.Tensor - Matrice di confusione 2x2\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Assicurati che siano sul CPU e siano float\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Inizializza la matrice di confusione\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.device' object is not callable"
     ]
    }
   ],
   "source": [
    "confusion_matrix = final_test(model,testloader,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_from_confusion(confusion_matrix: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calcola accuracy, precision, recall e f1 dalla matrice di confusione\n",
    "    Args:\n",
    "        confusion_matrix: torch.Tensor - Matrice 2x2 con [TN, FP], [FN, TP]\n",
    "    Returns:\n",
    "        dict con le metriche calcolate\n",
    "    \"\"\"\n",
    "    tn, fp = confusion_matrix[0]\n",
    "    fn, tp = confusion_matrix[1]\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy.item(),\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1': f1.item()\n",
    "    }\n",
    "\n",
    "metrics = get_metrics_from_confusion(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
