{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoK_TEzKw0cX"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2PIJs73w0cY"
   },
   "source": [
    "### Run on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bW7nDpRfw0cY",
    "outputId": "207757cf-b2e8-4ea4-d433-3ab173628f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'GourmetAI'...\n",
      "remote: Enumerating objects: 171, done.\u001b[K\n",
      "remote: Total 171 (delta 0), reused 0 (delta 0), pack-reused 171 (from 1)\u001b[K\n",
      "Receiving objects: 100% (171/171), 159.31 MiB | 10.39 MiB/s, done.\n",
      "Resolving deltas: 100% (59/59), done.\n",
      "/content/GourmetAI\n",
      "Updating files: 100% (75/75), done.\n",
      "Branch 'colab_test' set up to track remote branch 'colab_test' from 'origin'.\n",
      "Switched to a new branch 'colab_test'\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/Giovannicus/GourmetAI.git\n",
    "%cd GourmetAI\n",
    "!git checkout colab_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzUCo5fJdbO4",
    "outputId": "f8c9ccb2-b29d-49cd-cbe8-a7a4fb5c35a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "def download_and_unzip(url, extract_to=\".\"):\n",
    "\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    zip_path = os.path.join(extract_to, 'dataset.zip')\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(zip_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=128):\n",
    "            file.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    os.remove(zip_path)\n",
    "    print(f\"I file sono stati estratti in: {extract_to}\")\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYrW9IAYw0cZ"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ou1scoZfbltP",
    "outputId": "b4b39f7e-24af-4757-e3d0-7370e6bbd463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I file sono stati estratti in: .\n"
     ]
    }
   ],
   "source": [
    "download_and_unzip(\"https://proai-datasets.s3.eu-west-3.amazonaws.com/dataset_food_classification.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nJ2nqRlmdSGe"
   },
   "outputs": [],
   "source": [
    "class Transforms:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, *args, **kwargs):\n",
    "        return self.transforms(image=np.array(img))['image']\n",
    "\n",
    "transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.HorizontalFlip(),\n",
    "        A.Rotate(limit=10),\n",
    "        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "      ])\n",
    "\n",
    "transform2 = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wQzscBNVhTF1"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root='dataset/train', transform=Transforms(transform))\n",
    "\n",
    "valset = torchvision.datasets.ImageFolder(root='dataset/val', transform=Transforms(transform))\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root='dataset/test',transform=Transforms(transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGH1S9XphWP-",
    "outputId": "cf0ee053-46c0-4743-fa97-956a618062f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8960, 2240, 2800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tmRwbkfZhd6_"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_batches = 15\n",
    "num_samples = num_batches * batch_size\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Funzione per ottenere indici bilanciati\n",
    "def get_balanced_indices(dataset, num_samples):\n",
    "    # Raggruppiamo gli indici per classe\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    # Calcoliamo quanti campioni per classe vogliamo\n",
    "    num_classes = len(class_indices)\n",
    "    samples_per_class = num_samples // num_classes\n",
    "\n",
    "    # Selezioniamo in modo bilanciato\n",
    "    balanced_indices = []\n",
    "    for class_idx in class_indices:\n",
    "        indices = class_indices[class_idx]\n",
    "        random.shuffle(indices)\n",
    "        balanced_indices.extend(indices[:samples_per_class])\n",
    "\n",
    "    # Shuffle finale degli indici bilanciati\n",
    "    random.shuffle(balanced_indices)\n",
    "    return balanced_indices\n",
    "\n",
    "# Otteniamo indici bilanciati per train e val\n",
    "train_indices = get_balanced_indices(trainset, len(trainset))\n",
    "val_indices = get_balanced_indices(valset, len(valset))\n",
    "test_indices = get_balanced_indices(testset, len(testset))\n",
    "\n",
    "limited_trainset = Subset(trainset, train_indices)\n",
    "limited_valset = Subset(valset, val_indices)\n",
    "limited_testset = Subset(testset, test_indices)\n",
    "# Creiamo i dataloader\n",
    "trainloader = torch.utils.data.DataLoader(limited_trainset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)  # Manteniamo anche lo shuffle nel DataLoader\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(limited_valset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(limited_testset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset,\n",
    "#                                         batch_size=batch_size,\n",
    "#                                         shuffle=True,     # Shuffle per il training\n",
    "#                                         num_workers=4)    # Aggiungo worker per velocizzare il caricamento\n",
    "\n",
    "# valloader = torch.utils.data.DataLoader(valset,\n",
    "#                                       batch_size=batch_size,\n",
    "#                                       shuffle=False,      # No shuffle per validation\n",
    "#                                       num_workers=4)\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(testset,\n",
    "#                                        batch_size=batch_size,\n",
    "#                                        shuffle=False,     # No shuffle per test\n",
    "#                                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXGY6Edbw0cb",
    "outputId": "9943d503-71e2-40a4-ab34-479ec3dc1745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: tensor([640., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640.,\n",
      "        640., 640.])\n"
     ]
    }
   ],
   "source": [
    "label_counts = torch.zeros(14)  # Assuming `num_classes` is known\n",
    "for _, labels in trainloader:\n",
    "    for label in labels:\n",
    "        label_counts[label] += 1\n",
    "print(\"Label distribution:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwF-C6wnw0cb"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9b_GjsxhkaY",
    "outputId": "0b8b67e6-bad4-4be1-853b-9e30aff71391"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:05<00:00, 92.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 1, 1]               0\n",
      "           Linear-33                   [-1, 14]           7,182\n",
      "================================================================\n",
      "Total params: 14,721,870\n",
      "Trainable params: 7,182\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.40\n",
      "Params size (MB): 56.16\n",
      "Estimated Total Size (MB): 275.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vgg = vgg16(weights = VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "class VGGClassifier(nn.Module):\n",
    "    def __init__(self, vgg_backbone, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = vgg_backbone.features\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "vgg = VGGClassifier(vgg,512,14).to(device)\n",
    "summary(vgg,(3, 224, 224), device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey1811fTw0cb"
   },
   "source": [
    "## Training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af96L1gPw0cb"
   },
   "source": [
    "### logger + exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XQY4vOELw0cc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "def get_logger(ch_log_level=logging.ERROR,\n",
    "               fh_log_level=logging.INFO):\n",
    "    logging.shutdown()\n",
    "    importlib.reload(logging)  # Sostituito imp.reload con importlib.reload\n",
    "    logger = logging.getLogger(\"cheatsheet\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Console Handler\n",
    "    if ch_log_level:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(ch_log_level)\n",
    "        ch.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(ch)\n",
    "\n",
    "    # File Handler\n",
    "    if fh_log_level:\n",
    "        fh = logging.FileHandler('cheatsheet.log')\n",
    "        fh.setLevel(fh_log_level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class Experiment():\n",
    "    def __init__(self, name, root, logger=None):\n",
    "        self.name = name\n",
    "        self.root = os.path.join(root, name)\n",
    "        self.logger = logger\n",
    "        self.epoch = 1\n",
    "        self.best_val_loss = sys.maxsize\n",
    "        self.best_val_loss_epoch = 1\n",
    "        self.weights_dir = os.path.join(self.root, 'weights')\n",
    "        self.history_dir = os.path.join(self.root, 'history')\n",
    "        self.results_dir = os.path.join(self.root, 'results')\n",
    "        self.latest_weights = os.path.join(self.weights_dir, 'latest_weights.pth')\n",
    "        self.latest_optimizer = os.path.join(self.weights_dir, 'latest_optim.pth')\n",
    "        self.best_weights_path = self.latest_weights\n",
    "        self.best_optimizer_path = self.latest_optimizer\n",
    "        self.train_history_fpath = os.path.join(self.history_dir, 'train.csv')\n",
    "        self.val_history_fpath = os.path.join(self.history_dir, 'val.csv')\n",
    "        self.test_history_fpath = os.path.join(self.history_dir, 'test.csv')\n",
    "        self.loss_history = {\n",
    "            'train': np.array([]),\n",
    "            'val': np.array([]),\n",
    "            'test': np.array([])\n",
    "        }\n",
    "        self.precision_history = {\n",
    "            'train': np.array([]),\n",
    "            'val': np.array([]),\n",
    "            'test': np.array([])\n",
    "        }\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.logger:\n",
    "            self.logger.info(msg)\n",
    "\n",
    "    def init(self):\n",
    "        self.log(\"Creating new experiment\")\n",
    "        self.init_dirs()\n",
    "        self.init_history_files()\n",
    "\n",
    "    def resume(self, model, optim, weights_fpath=None, optim_path=None):\n",
    "        self.log(\"Resuming existing experiment\")\n",
    "        if weights_fpath is None:\n",
    "            weights_fpath = self.latest_weights\n",
    "        if optim_path is None:\n",
    "            optim_path = self.latest_optimizer\n",
    "\n",
    "        model, state = self.load_weights(model, weights_fpath)\n",
    "        optim = self.load_optimizer(optim, optim_path)\n",
    "\n",
    "        self.best_val_loss = state['best_val_loss']\n",
    "        self.best_val_loss_epoch = state['best_val_loss_epoch']\n",
    "        self.epoch = state['last_epoch'] + 1\n",
    "        self.load_history_from_file('train')\n",
    "        self.load_history_from_file('val')\n",
    "\n",
    "        return model, optim\n",
    "\n",
    "    def init_dirs(self):\n",
    "        os.makedirs(self.weights_dir, exist_ok=True)\n",
    "        os.makedirs(self.history_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "\n",
    "    def init_history_files(self):\n",
    "        Path(self.train_history_fpath).touch()\n",
    "        Path(self.val_history_fpath).touch()\n",
    "        Path(self.test_history_fpath).touch()\n",
    "\n",
    "\n",
    "    def load_history_from_file(self, dset_type):\n",
    "        fpath = os.path.join(self.history_dir, dset_type + '.csv')\n",
    "        try:\n",
    "            data = np.loadtxt(fpath, delimiter=',').reshape(-1, 3)\n",
    "            self.loss_history[dset_type] = data[:, 1]\n",
    "            self.precision_history[dset_type] = data[:, 2]\n",
    "        except:\n",
    "            self.loss_history[dset_type] = np.array([])\n",
    "            self.precision_history[dset_type] = np.array([])\n",
    "\n",
    "\n",
    "    def append_history_to_file(self, dset_type, loss, precision):\n",
    "        fpath = os.path.join(self.history_dir, dset_type + '.csv')\n",
    "        with open(fpath, 'a') as f:\n",
    "            f.write('{},{},{}\\n'.format(self.epoch, loss, precision))\n",
    "\n",
    "    def save_history(self, dset_type, loss, precision):\n",
    "        self.loss_history[dset_type] = np.append(self.loss_history[dset_type], loss)\n",
    "        self.precision_history[dset_type] = np.append(self.precision_history[dset_type], precision)\n",
    "        self.append_history_to_file(dset_type, loss, precision)\n",
    "\n",
    "        if dset_type == 'val' and self.is_best_loss(loss):\n",
    "            self.best_val_loss = loss\n",
    "            self.best_val_loss_epoch = self.epoch\n",
    "\n",
    "        if dset_type == 'val':\n",
    "            self.plot_and_save_history()\n",
    "\n",
    "    def is_best_loss(self, loss):\n",
    "        return loss < self.best_val_loss\n",
    "\n",
    "    def save_weights(self, model, trn_loss, val_loss, trn_precision, val_precision):\n",
    "        weights_fname = self.name + '-weights-%d-%.3f-%.3f-%.3f-%.3f.pth' % (\n",
    "            self.epoch, trn_loss, trn_precision, val_loss, val_precision)\n",
    "        weights_fpath = os.path.join(self.weights_dir, weights_fname)\n",
    "        torch.save({\n",
    "            'last_epoch': self.epoch,\n",
    "            'trn_loss': trn_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'trn_precision': trn_precision,\n",
    "            'val_precision': val_precision,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_val_loss_epoch': self.best_val_loss_epoch,\n",
    "            'experiment': self.name,\n",
    "            'state_dict': model.state_dict()\n",
    "        }, weights_fpath)\n",
    "        shutil.copyfile(weights_fpath, self.latest_weights)\n",
    "        if self.is_best_loss(val_loss):\n",
    "            self.best_weights_path = weights_fpath\n",
    "\n",
    "    def load_weights(self, model, fpath):\n",
    "        self.log(\"loading weights '{}'\".format(fpath))\n",
    "        state = torch.load(fpath,map_location=torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        self.log(\"loaded weights from experiment %s (last_epoch %d, trn_loss %s, trn_precision %s, val_loss %s, val_precision %s)\" % (\n",
    "            self.name, state['last_epoch'], state['trn_loss'],\n",
    "            state['trn_precision'], state['val_loss'], state['val_precision']))\n",
    "        return model, state\n",
    "\n",
    "    def save_optimizer(self, optimizer, val_loss):\n",
    "        optim_fname = self.name + '-optim-%d.pth' % (self.epoch)\n",
    "        optim_fpath = os.path.join(self.weights_dir, optim_fname)\n",
    "        torch.save({\n",
    "            'last_epoch': self.epoch,\n",
    "            'experiment': self.name,\n",
    "            'state_dict': optimizer.state_dict()\n",
    "        }, optim_fpath)\n",
    "        shutil.copyfile(optim_fpath, self.latest_optimizer)\n",
    "        if self.is_best_loss(val_loss):\n",
    "            self.best_optimizer_path = optim_fpath\n",
    "\n",
    "    def load_optimizer(self, optimizer, fpath):\n",
    "        self.log(\"loading optimizer '{}'\".format(fpath))\n",
    "        optim = torch.load(fpath,map_location=torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        optimizer.load_state_dict(optim['state_dict'])\n",
    "        self.log(\"loaded optimizer from session {}, last_epoch {}\"\n",
    "                 .format(optim['experiment'], optim['last_epoch']))\n",
    "        return optimizer\n",
    "\n",
    "    def plot_and_save_history(self):\n",
    "        if not hasattr(self, 'monitor'):\n",
    "            self.monitor = ColabTrainingMonitor(figsize=(12, 8))\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(self.train_history_fpath) and os.path.exists(self.val_history_fpath):\n",
    "                train_data = np.loadtxt(self.train_history_fpath, delimiter=',')\n",
    "                val_data = np.loadtxt(self.val_history_fpath, delimiter=',')\n",
    "\n",
    "                if train_data.ndim == 1:\n",
    "                    train_data = train_data.reshape(1, -1)\n",
    "                if val_data.ndim == 1:\n",
    "                    val_data = val_data.reshape(1, -1)\n",
    "\n",
    "                latest_epoch = int(train_data[-1, 0])\n",
    "                train_loss = train_data[-1, 1]\n",
    "                train_precision = train_data[-1, 2]\n",
    "                val_loss = val_data[-1, 1]\n",
    "                val_precision = val_data[-1, 2]\n",
    "\n",
    "                self.monitor.update(\n",
    "                    epoch=latest_epoch,\n",
    "                    train_loss=train_loss,\n",
    "                    val_loss=val_loss,\n",
    "                    train_precision=train_precision,\n",
    "                    val_precision=val_precision\n",
    "                )\n",
    "\n",
    "                self.monitor.save(os.path.join(self.history_dir, 'training_progress.png'))\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.warning(f\"Error updating plot: {e}\")\n",
    "\n",
    "    # def plot_and_save_history(self):\n",
    "    #     if not hasattr(self, 'interactive_plot'):\n",
    "    #         self.interactive_plot = InteractivePlot(\n",
    "    #             self.train_history_fpath,\n",
    "    #             self.val_history_fpath,\n",
    "    #             self.history_dir\n",
    "    #         )\n",
    "    #         # Avvia il plotting in un thread separato\n",
    "    #         from threading import Thread\n",
    "    #         self.plot_thread = Thread(target=self.interactive_plot.run, args=(1,), daemon=True)\n",
    "    #         self.plot_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3uMjOO4w0cc"
   },
   "source": [
    "### static interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w5VNckYAw0cc"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class SimpleTrainingMonitor:\n",
    "    def __init__(self, train_history_fpath, val_history_fpath):\n",
    "        self.train_history_fpath = train_history_fpath\n",
    "        self.val_history_fpath = val_history_fpath\n",
    "\n",
    "    def create_figure(self):\n",
    "        \"\"\"Create and return the plotly figure\"\"\"\n",
    "        # Leggi i dati\n",
    "        train_df = pd.read_csv(self.train_history_fpath, names=['epoch', 'loss', 'precision'])\n",
    "        val_df = pd.read_csv(self.val_history_fpath, names=['epoch', 'loss', 'precision'])\n",
    "\n",
    "        # Crea subplot\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=('Loss Over Time', 'Precision Over Time'),\n",
    "            vertical_spacing=0.15\n",
    "        )\n",
    "\n",
    "        # Aggiungi tracce per il training\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=train_df['epoch'], y=train_df['loss'],\n",
    "                      name='Train Loss', mode='lines+markers',\n",
    "                      line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=train_df['epoch'], y=train_df['precision'],\n",
    "                      name='Train Precision', mode='lines+markers',\n",
    "                      line=dict(color='blue')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Aggiungi tracce per la validation\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=val_df['epoch'], y=val_df['loss'],\n",
    "                      name='Val Loss', mode='lines+markers',\n",
    "                      line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=val_df['epoch'], y=val_df['precision'],\n",
    "                      name='Val Precision', mode='lines+markers',\n",
    "                      line=dict(color='red')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Aggiorna il layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            title_text=\"Training Progress\",\n",
    "            title_x=0.5,\n",
    "            plot_bgcolor='white',  # sfondo bianco\n",
    "            paper_bgcolor='white'\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgrey',\n",
    "            gridwidth=0.2,\n",
    "            griddash='dot',  # oppure usa 'dot' per punti invece che trattini\n",
    "            # oppure usa un pattern personalizzato:\n",
    "            # griddash='2,2',  # numeri più piccoli = trattini più corti e più vicini\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            linewidth=1,\n",
    "            title_text=\"Epoch\"\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgrey',\n",
    "            gridwidth=0.2,\n",
    "            griddash='dot',  # oppure 'dot' per punti\n",
    "            # oppure griddash='2,2',\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # Imposta specificamente gli assi y\n",
    "        fig.update_yaxes(title_text=\"Loss\", type=\"log\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Precision\", row=2, col=1)\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Display the plot\"\"\"\n",
    "        fig = self.create_figure()\n",
    "        return fig  # Questo permetterà a Jupyter di mostrare il plot\n",
    "\n",
    "def plot_training_history(experiment_dir):\n",
    "    \"\"\"Helper function to quickly plot training history\"\"\"\n",
    "    train_path = os.path.join(experiment_dir, \"history\", \"train.csv\")\n",
    "    val_path = os.path.join(experiment_dir, \"history\", \"val.csv\")\n",
    "\n",
    "    monitor = SimpleTrainingMonitor(train_path, val_path)\n",
    "    return monitor.plot()  # Ritorna la figura invece di mostrarla direttamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l3XB3Tsw0cc"
   },
   "source": [
    "### colab interaactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfgSyjTGw0cc"
   },
   "outputs": [],
   "source": [
    "# Monitor per Colab\n",
    "class ColabTrainingMonitor:\n",
    "    def __init__(self, figsize=(12, 8)):\n",
    "        self.figsize = figsize\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_precisions = []\n",
    "        self.val_precisions = []\n",
    "        self.epochs = []\n",
    "\n",
    "        # Setup plot style\n",
    "        plt.style.use('default')\n",
    "\n",
    "    def update(self, epoch, train_loss, val_loss, train_precision, val_precision):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_precisions.append(train_precision)\n",
    "        self.val_precisions.append(val_precision)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        self.plot()\n",
    "\n",
    "    def plot(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=self.figsize)\n",
    "        fig.suptitle('Training Progress', fontsize=16, y=1.02)\n",
    "\n",
    "        # Plot Loss\n",
    "        ax1.plot(self.epochs, self.train_losses, 'b-o', label='Training Loss',\n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax1.plot(self.epochs, self.val_losses, 'r-o', label='Validation Loss',\n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax1.set_title('Loss Over Time', pad=10)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax1.legend(loc='upper right', frameon=True)\n",
    "\n",
    "        # Annotate last values\n",
    "        if self.train_losses:\n",
    "            ax1.annotate(f'{self.train_losses[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.train_losses[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "            ax1.annotate(f'{self.val_losses[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.val_losses[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "\n",
    "        # Plot Precision\n",
    "        ax2.plot(self.epochs, self.train_precisions, 'b-o', label='Training Precision',\n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax2.plot(self.epochs, self.val_precisions, 'r-o', label='Validation Precision',\n",
    "                markersize=4, linewidth=2, alpha=0.8)\n",
    "        ax2.set_title('Precision Over Time', pad=10)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax2.legend(loc='lower right', frameon=True)\n",
    "        ax2.set_ylim(0, 1)  # precision è sempre tra 0 e 1\n",
    "\n",
    "        # Annotate last values\n",
    "        if self.train_precisions:\n",
    "            ax2.annotate(f'{self.train_precisions[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.train_precisions[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "            ax2.annotate(f'{self.val_precisions[-1]:.4f}',\n",
    "                        (self.epochs[-1], self.val_precisions[-1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the current plot to a file\"\"\"\n",
    "        self.plot()\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    def get_current_metrics(self):\n",
    "        if not self.epochs:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            'epoch': self.epochs[-1],\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'train_precision': self.train_precisions[-1],\n",
    "            'val_precision': self.val_precisions[-1]\n",
    "        }\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Cleanup method\"\"\"\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr-uwdIUw0cd"
   },
   "source": [
    "### set train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aZR9ri3mw0cd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "\n",
    "def train(net, dataloader, criterion, optimizer, epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    net.train()\n",
    "    n_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    total_precision = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        ## Forward Pass\n",
    "        output = net(inputs)\n",
    "\n",
    "        ## Clear Gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        ## Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions and compute precision\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        precision = get_precision(preds, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_precision += precision\n",
    "\n",
    "    mean_loss = total_loss / n_batches\n",
    "    mean_precision = total_precision / n_batches\n",
    "    return mean_loss, mean_precision\n",
    "\n",
    "def test(net, test_loader, criterion, epoch=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_precision = 0\n",
    "    n_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            batch_precision = get_precision(preds, target)\n",
    "            test_precision += batch_precision\n",
    "\n",
    "    test_loss /= n_batches\n",
    "    test_precision /= n_batches\n",
    "\n",
    "    print(f\"\\nFinal validation metrics:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "\n",
    "    return test_loss, test_precision\n",
    "\n",
    "def get_predictions(model_output):\n",
    "    \"\"\"Ottiene le predizioni dal output del modello\"\"\"\n",
    "    device = model_output.device  # mantieni lo stesso device dell'output\n",
    "    val, idx = torch.max(model_output, dim=1)\n",
    "    return idx  # mantieni sul device originale\n",
    "\n",
    "def get_precision(preds: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "\n",
    "    device = targets.device\n",
    "    preds = preds.to(device)\n",
    "\n",
    "    preds = manual_one_hot(preds,14)\n",
    "    targets = manual_one_hot(targets,14)\n",
    "    # Inizializza liste per le metriche\n",
    "    precisions = []\n",
    "\n",
    "    # Calcola la precisione per ogni classe\n",
    "    num_classes = preds.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        class_preds = preds[:, class_idx]\n",
    "        class_targets = targets[:, class_idx]\n",
    "\n",
    "        # Calcola true positives e total predictions\n",
    "        true_positives = (class_preds * class_targets).sum()\n",
    "        false_positives = (class_preds * (1 - class_targets)).sum()\n",
    "        total_predicted = true_positives+false_positives\n",
    "\n",
    "        # Calcola la precisione per questa classe se ci sono predizioni\n",
    "        if total_predicted > 0:\n",
    "            class_precision = true_positives / total_predicted\n",
    "            precisions.append(class_precision)\n",
    "\n",
    "        else:\n",
    "            class_precision = torch.nan\n",
    "            precisions.append(class_precision)\n",
    "\n",
    "    precision_tensor = torch.tensor(precisions)\n",
    "    mean_precision = precision_tensor.nanmean().item()\n",
    "\n",
    "    return mean_precision\n",
    "\n",
    "# Queste funzioni rimangono invariate perché non sono legate alla metrica\n",
    "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
    "    \"\"\"Sets the learning rate to the initially\n",
    "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
    "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_uniform(m.weight)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "def save_weights(model, weights_dir, epoch):\n",
    "    weights_fname = 'weights-%d.pth' % (epoch)\n",
    "    weights_fpath = os.path.join(weights_dir, weights_fname)\n",
    "    torch.save({'state_dict': model.state_dict()}, weights_fpath)\n",
    "\n",
    "def load_weights(model, fpath):\n",
    "    state = torch.load(fpath)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "\n",
    "def manual_one_hot(tensor, num_classes=None):\n",
    "    if num_classes is None:\n",
    "        num_classes = tensor.max() + 1\n",
    "    one_hot = torch.zeros((tensor.size(0), num_classes),device=device)\n",
    "    one_hot.scatter_(1, tensor.unsqueeze(1), 1)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w2K9EA0w0cd"
   },
   "source": [
    "### New Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLuBLnzGw0cd"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "MAX_PATIENCE = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_DECAY = 0.99\n",
    "DECAY_LR_EVERY_N_EPOCHS = 2\n",
    "EXPERIMENT_NAME = 'exp1'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = vgg.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "logger = get_logger(ch_log_level=logging.INFO, fh_log_level=logging.INFO)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print('  + Number of params: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "exp = Experiment(EXPERIMENT_NAME, \"exps\", logger)\n",
    "#shutil.rmtree(\"exps\\exp1\")\n",
    "# Create New Experiment\n",
    "exp.init()\n",
    "monitor = ColabTrainingMonitor()\n",
    "import time\n",
    "\n",
    "for epoch in range(exp.epoch, exp.epoch+N_EPOCHS):\n",
    "    since = time.time()\n",
    "\n",
    "    ### Train ###\n",
    "    trn_loss, trn_precision = train(model, trainloader, criterion, optimizer, epoch)\n",
    "    logger.info('Epoch {:d}: Train - Loss: {:.4f}\\tPrecision: {:.4f}'.format(\n",
    "        epoch, trn_loss, trn_precision))\n",
    "    time_elapsed = time.time() - since\n",
    "    logger.info('Train Time {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    ### Test ###\n",
    "    val_loss, val_precision = test(model, valloader, criterion, epoch)\n",
    "    logger.info('Val - Loss: {:.4f}, Precision: {:.4f}'.format(\n",
    "        val_loss, val_precision))\n",
    "    time_elapsed = time.time() - since\n",
    "    logger.info('Total Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    monitor.update(\n",
    "        epoch=epoch,\n",
    "        train_loss=trn_loss,\n",
    "        val_loss=val_loss,\n",
    "        train_precision=trn_precision,\n",
    "        val_precision=val_precision\n",
    "    )\n",
    "\n",
    "    ### Save Metrics ###\n",
    "    exp.save_history('train', trn_loss, trn_precision)\n",
    "    exp.save_history('val', val_loss, val_precision)\n",
    "\n",
    "    ### Checkpoint ###\n",
    "    exp.save_weights(model, trn_loss, val_loss, trn_precision, val_precision)\n",
    "    exp.save_optimizer(optimizer, val_loss)\n",
    "\n",
    "    ## Early Stopping ##\n",
    "    if (epoch - exp.best_val_loss_epoch) > MAX_PATIENCE:\n",
    "        logger.info(f\"Early Stopping at epoch {epoch} since no better loss found since epoch {exp.best_val_loss_epoch}\")\n",
    "        break\n",
    "\n",
    "    ### Adjust Lr ###\n",
    "    adjust_learning_rate(LEARNING_RATE, LR_DECAY, optimizer,\n",
    "                         epoch, DECAY_LR_EVERY_N_EPOCHS)\n",
    "\n",
    "    exp.epoch += 1\n",
    "\n",
    "try:\n",
    "    monitor.save(os.path.join(exp.history_dir, 'final_training_progress.png'))\n",
    "    monitor.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwJQ71Muw0cd"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"exp1.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")  # Estrae nella directory corrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PjcYODjw0cd"
   },
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "5L4hK5FOw0cd",
    "outputId": "6c8fc30c-ba14-48f5-a0fe-80fea25501bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"81e203f3-e26a-464d-8f00-b0d2ce4cd219\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"81e203f3-e26a-464d-8f00-b0d2ce4cd219\")) {                    Plotly.newPlot(                        \"81e203f3-e26a-464d-8f00-b0d2ce4cd219\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Train Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[2.0229532735688345,1.3494205372674124,1.097415392739432,0.9669795393943786,0.8832692520959037,0.8215823403426579,0.7822024524211884,0.7462329030036926,0.7132247269153595,0.6919813249792371,0.6709802465779441,0.6497110681874412,0.6358537256717682,0.6226951705557959,0.6032812540020261,0.5953331576926367,0.5890365732567651,0.5769379773310253,0.5677769247974668,0.5584123815808978,0.5510100960731507,0.5466924497059413,0.5372813880443573,0.5358492800167629,0.5225674931492125,0.5185524110283171,0.5104323140212468,0.5083380137171064,0.5026400434119361,0.4970872108425412],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Train Precision\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[0.5234681258776358,0.7057939086641584,0.735838223355157,0.7476783743926457,0.775221346105848,0.7824589814458575,0.7901264786720276,0.7934940874576568,0.80538461293493,0.8098329799515861,0.8122287579945155,0.8178505046027047,0.8213189457144056,0.8218315277780806,0.8290947786399296,0.8266719247613634,0.8314164876937866,0.8343684443405697,0.8293601027556828,0.8371197879314423,0.8389147017683302,0.8411034183842795,0.843681994506291,0.8420234535421643,0.851783515725817,0.8477509694440024,0.8551594487258366,0.8532848341124398,0.8528815593038287,0.8543217190674373],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Val Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[1.594865812195672,1.241813666290707,1.0844997002018824,0.9929949012067584,0.9308792584472232,0.8747847312026553,0.8408482604556613,0.8183648222022586,0.7982479830582937,0.7763034866915809,0.761910253100925,0.7573864029513465,0.7389473186598884,0.7238157060411241,0.7217933237552643,0.7033550341924032,0.6871284445126852,0.6959854861100515,0.6914075911045074,0.6819814907179939,0.6791924834251404,0.6698530581262376,0.672434839937422,0.6575950351026323,0.6655533230966992,0.6585836874114143,0.6618831422593858,0.6580426692962646,0.6429681148793962,0.6513145532872942],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Val Precision\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y\":[0.6458376381132338,0.6992929213576846,0.7176814609103732,0.7381439606348673,0.7450825174649557,0.7578935523827871,0.7653312087059021,0.7648385961850485,0.7740530603461795,0.7808139622211456,0.7868409752845764,0.7770640552043915,0.7832246687677171,0.7768451439009773,0.780700147151947,0.7970755894978842,0.8027176691426171,0.7905768752098083,0.7870072027047476,0.7991315821806589,0.7961920234892104,0.7935494515630934,0.7942302525043488,0.8006013532479604,0.7891319427225325,0.7884893980291154,0.7907672425111135,0.7892610960536532,0.8076005743609534,0.7958064410421584],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"},\"showgrid\":true,\"gridcolor\":\"lightgrey\",\"gridwidth\":0.2,\"griddash\":\"dot\",\"zeroline\":false,\"linecolor\":\"black\",\"linewidth\":1},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.575,1.0],\"showgrid\":true,\"gridcolor\":\"lightgrey\",\"gridwidth\":0.2,\"griddash\":\"dot\",\"zeroline\":false,\"linecolor\":\"black\",\"linewidth\":1,\"title\":{\"text\":\"Loss\"},\"type\":\"log\"},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"},\"showgrid\":true,\"gridcolor\":\"lightgrey\",\"gridwidth\":0.2,\"griddash\":\"dot\",\"zeroline\":false,\"linecolor\":\"black\",\"linewidth\":1},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.425],\"showgrid\":true,\"gridcolor\":\"lightgrey\",\"gridwidth\":0.2,\"griddash\":\"dot\",\"zeroline\":false,\"linecolor\":\"black\",\"linewidth\":1,\"title\":{\"text\":\"Precision\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Loss Over Time\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Precision Over Time\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.425,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Training Progress\",\"x\":0.5},\"height\":800,\"showlegend\":true,\"plot_bgcolor\":\"white\",\"paper_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('81e203f3-e26a-464d-8f00-b0d2ce4cd219');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_training_history('exps/exp1')\n",
    "fig  # In Jupyter, l'ultima espressione viene visualizzata automaticamente\n",
    "\n",
    "# Metodo 2: usando la classe direttamente\n",
    "monitor = SimpleTrainingMonitor(\n",
    "    train_history_fpath='exps/exp1/history/train.csv',\n",
    "    val_history_fpath='exps/exp1/history/val.csv'\n",
    ")\n",
    "fig = monitor.plot()\n",
    "fig  # Mostra il plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLZilcssw0cd"
   },
   "source": [
    "## final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwAFfaAmw0cd",
    "outputId": "92670c4f-0103-4cc5-b0cd-02c1a7620223"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-c7cefe42d424>:154: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "<ipython-input-10-c7cefe42d424>:175: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(name=\"exp1\", root=\"exps\")\n",
    "\n",
    "# Per caricare i migliori pesi (invece degli ultimi)\n",
    "model, optimizer = experiment.resume(\n",
    "    model=model,  # il tuo modello vuoto\n",
    "    optim=optimizer,  # il tuo ottimizzatore inizializzato\n",
    "    weights_fpath=experiment.best_weights_path,  # usa i migliori pesi invece degli ultimi\n",
    "    optim_path=experiment.best_optimizer_path  # usa il miglior ottimizzatore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dDzHlT9qw0cd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_confusion_matrix(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate confusion matrix given predictions and targets.\n",
    "\n",
    "    Args:\n",
    "        preds: torch.Tensor - Model predictions (already processed with argmax if necessary)\n",
    "        targets: torch.Tensor - Ground truth targets\n",
    "\n",
    "    Returns:\n",
    "        confusion_matrix: torch.Tensor - Confusion matrix of shape (num_classes x num_classes)\n",
    "    \"\"\"\n",
    "    confusion_matrix = torch.zeros(14, 14)\n",
    "    for t, p in zip(targets.view(-1), preds.view(-1)):\n",
    "        confusion_matrix[t.long(), p.long()] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_metrics(confusion_matrix: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate various metrics from the confusion matrix using only PyTorch operations.\n",
    "\n",
    "    Args:\n",
    "        confusion_matrix: torch.Tensor - Confusion matrix of shape (num_classes x num_classes)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "    # Per-class metrics\n",
    "    class_precision = confusion_matrix.diag() / confusion_matrix.sum(dim=1).clamp(min=1e-10)\n",
    "    class_recall = confusion_matrix.diag() / confusion_matrix.sum(dim=0).clamp(min=1e-10)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall + 1e-10)\n",
    "\n",
    "    # Calculate macro averages\n",
    "    metrics = {\n",
    "        'macro_precision': float(torch.mean(class_precision)),\n",
    "        'macro_recall': float(torch.mean(class_recall)),\n",
    "        'macro_f1': float(torch.mean(class_f1)),\n",
    "        'accuracy': float(confusion_matrix.diag().sum() / confusion_matrix.sum())\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def final_test(net, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Perform final evaluation of the model on test data.\n",
    "\n",
    "    Args:\n",
    "        net: torch.nn.Module - The neural network model\n",
    "        test_loader: torch.utils.data.DataLoader - Test data loader\n",
    "        criterion: torch.nn.Module - Loss function\n",
    "\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, metrics_dict)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    n_batches = len(test_loader)\n",
    "\n",
    "    # Initialize tensors for all predictions and targets\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = net(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "\n",
    "            # Store predictions and targets\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(target.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    test_loss /= n_batches\n",
    "    confusion_matrix = get_confusion_matrix(all_preds, all_targets)\n",
    "    metrics = calculate_metrics(confusion_matrix)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nFinal validation metrics:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    print(\"\\nMetrics from confusion matrix:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return confusion_matrix, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkWvrLQnw0ce",
    "outputId": "04355b79-b9c7-44a8-e58e-ebbf75f70fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final validation metrics:\n",
      "Loss: 0.6565\n",
      "\n",
      "Confusion Matrix:\n",
      "tensor([[174.,   5.,   1.,   3.,   4.,   2.,   5.,   3.,   1.,   0.,   0.,   1.,\n",
      "           1.,   0.],\n",
      "        [ 10., 172.,   0.,   2.,   0.,   4.,   5.,   2.,   0.,   0.,   3.,   0.,\n",
      "           2.,   0.],\n",
      "        [  1.,   1., 188.,   0.,   4.,   0.,   0.,   2.,   2.,   0.,   0.,   1.,\n",
      "           0.,   1.],\n",
      "        [  3.,   2.,   0., 174.,   2.,   2.,  12.,   3.,   0.,   0.,   1.,   1.,\n",
      "           0.,   0.],\n",
      "        [  6.,   1.,   5.,   8., 145.,   6.,  13.,  10.,   3.,   1.,   1.,   0.,\n",
      "           1.,   0.],\n",
      "        [  3.,   1.,   6.,   3.,   9., 163.,   7.,   6.,   0.,   1.,   0.,   1.,\n",
      "           0.,   0.],\n",
      "        [ 13.,   6.,   4.,   5.,  12.,   6., 131.,  15.,   1.,   1.,   2.,   2.,\n",
      "           1.,   1.],\n",
      "        [  9.,   4.,   5.,   2.,   4.,   4.,  20., 145.,   0.,   2.,   2.,   0.,\n",
      "           3.,   0.],\n",
      "        [  1.,   2.,   3.,   3.,   1.,   0.,   0.,   0., 148.,  14.,   3.,   9.,\n",
      "          12.,   4.],\n",
      "        [  1.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,  14., 166.,   3.,   6.,\n",
      "           4.,   5.],\n",
      "        [  0.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   8.,   0., 164.,   2.,\n",
      "          18.,   5.],\n",
      "        [  0.,   0.,   2.,   1.,   0.,   1.,   0.,   0.,  18.,  14.,   5., 152.,\n",
      "           1.,   6.],\n",
      "        [  4.,   0.,   0.,   0.,   0.,   1.,   0.,   2.,  14.,   2.,  12.,   3.,\n",
      "         156.,   6.],\n",
      "        [  4.,   0.,   3.,   0.,   1.,   0.,   0.,   0.,   7.,   2.,   3.,   9.,\n",
      "           6., 165.]])\n",
      "\n",
      "Metrics from confusion matrix:\n",
      "macro_precision: 0.8011\n",
      "macro_recall: 0.8018\n",
      "macro_f1: 0.8007\n",
      "accuracy: 0.8011\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, metrics = final_test(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "isJOlduA27QB",
    "outputId": "d629668c-2177-4bc8-e14a-72f31a5458a7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8fc3e26b30dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "_LOfm46R1Xfe",
    "outputId": "3ae31433-60bb-4f7c-8b34-55476bf5b7e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"bda52074-3edf-4890-9c2c-48a3ea0c04e2\" class=\"plotly-graph-div\" style=\"height:800px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bda52074-3edf-4890-9c2c-48a3ea0c04e2\")) {                    Plotly.newPlot(                        \"bda52074-3edf-4890-9c2c-48a3ea0c04e2\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"hoverongaps\":false,\"hovertemplate\":\"Vero: %{y}\\u003cbr\\u003ePredetto: %{x}\\u003cbr\\u003eValore: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"text\":[[174.0,5.0,1.0,3.0,4.0,2.0,5.0,3.0,1.0,0.0,0.0,1.0,1.0,0.0],[10.0,172.0,0.0,2.0,0.0,4.0,5.0,2.0,0.0,0.0,3.0,0.0,2.0,0.0],[1.0,1.0,188.0,0.0,4.0,0.0,0.0,2.0,2.0,0.0,0.0,1.0,0.0,1.0],[3.0,2.0,0.0,174.0,2.0,2.0,12.0,3.0,0.0,0.0,1.0,1.0,0.0,0.0],[6.0,1.0,5.0,8.0,145.0,6.0,13.0,10.0,3.0,1.0,1.0,0.0,1.0,0.0],[3.0,1.0,6.0,3.0,9.0,163.0,7.0,6.0,0.0,1.0,0.0,1.0,0.0,0.0],[13.0,6.0,4.0,5.0,12.0,6.0,131.0,15.0,1.0,1.0,2.0,2.0,1.0,1.0],[9.0,4.0,5.0,2.0,4.0,4.0,20.0,145.0,0.0,2.0,2.0,0.0,3.0,0.0],[1.0,2.0,3.0,3.0,1.0,0.0,0.0,0.0,148.0,14.0,3.0,9.0,12.0,4.0],[1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,14.0,166.0,3.0,6.0,4.0,5.0],[0.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,164.0,2.0,18.0,5.0],[0.0,0.0,2.0,1.0,0.0,1.0,0.0,0.0,18.0,14.0,5.0,152.0,1.0,6.0],[4.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,14.0,2.0,12.0,3.0,156.0,6.0],[4.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,7.0,2.0,3.0,9.0,6.0,165.0]],\"textfont\":{\"size\":14},\"texttemplate\":\"%{z}\",\"x\":[\"Baked Potato\",\"Crispy Chicken\",\"Donut\",\"Fries\",\"Hot Dog\",\"Sandwich\",\"Taco\",\"Taquito\",\"apple_pie\",\"cheesecake\",\"chicken_curry\",\"ice_cream\",\"omelette\",\"sushi\"],\"y\":[\"Baked Potato\",\"Crispy Chicken\",\"Donut\",\"Fries\",\"Hot Dog\",\"Sandwich\",\"Taco\",\"Taquito\",\"apple_pie\",\"cheesecake\",\"chicken_curry\",\"ice_cream\",\"omelette\",\"sushi\"],\"z\":[[174.0,5.0,1.0,3.0,4.0,2.0,5.0,3.0,1.0,0.0,0.0,1.0,1.0,0.0],[10.0,172.0,0.0,2.0,0.0,4.0,5.0,2.0,0.0,0.0,3.0,0.0,2.0,0.0],[1.0,1.0,188.0,0.0,4.0,0.0,0.0,2.0,2.0,0.0,0.0,1.0,0.0,1.0],[3.0,2.0,0.0,174.0,2.0,2.0,12.0,3.0,0.0,0.0,1.0,1.0,0.0,0.0],[6.0,1.0,5.0,8.0,145.0,6.0,13.0,10.0,3.0,1.0,1.0,0.0,1.0,0.0],[3.0,1.0,6.0,3.0,9.0,163.0,7.0,6.0,0.0,1.0,0.0,1.0,0.0,0.0],[13.0,6.0,4.0,5.0,12.0,6.0,131.0,15.0,1.0,1.0,2.0,2.0,1.0,1.0],[9.0,4.0,5.0,2.0,4.0,4.0,20.0,145.0,0.0,2.0,2.0,0.0,3.0,0.0],[1.0,2.0,3.0,3.0,1.0,0.0,0.0,0.0,148.0,14.0,3.0,9.0,12.0,4.0],[1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,14.0,166.0,3.0,6.0,4.0,5.0],[0.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,164.0,2.0,18.0,5.0],[0.0,0.0,2.0,1.0,0.0,1.0,0.0,0.0,18.0,14.0,5.0,152.0,1.0,6.0],[4.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,14.0,2.0,12.0,3.0,156.0,6.0],[4.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,7.0,2.0,3.0,9.0,6.0,165.0]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"font\":{\"size\":24},\"text\":\"Matrice di Confusione\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"xaxis\":{\"title\":{\"text\":\"Classe Predetta\"},\"side\":\"bottom\"},\"yaxis\":{\"title\":{\"text\":\"Classe Vera\"},\"autorange\":\"reversed\"},\"width\":800,\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bda52074-3edf-4890-9c2c-48a3ea0c04e2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix: torch.Tensor, class_names: list):\n",
    "    \"\"\"\n",
    "    Visualizza la confusion matrix usando Plotly con valori assoluti.\n",
    "\n",
    "    Args:\n",
    "        confusion_matrix: torch.Tensor - La matrice di confusione\n",
    "    \"\"\"\n",
    "    # Converti a numpy per plotly\n",
    "    cm = confusion_matrix.cpu().numpy()\n",
    "\n",
    "    # Crea la heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='Blues',\n",
    "        text=cm,\n",
    "        texttemplate=\"%{z}\",\n",
    "        textfont={\"size\": 14},\n",
    "        hoverongaps=False,\n",
    "        hovertemplate=\"Vero: %{y}<br>Predetto: %{x}<br>Valore: %{z}<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "    # Aggiorna il layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'Matrice di Confusione',\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': {'size': 24}\n",
    "        },\n",
    "        xaxis_title=\"Classe Predetta\",\n",
    "        yaxis_title=\"Classe Vera\",\n",
    "        xaxis={'side': 'bottom'},\n",
    "        width=800,\n",
    "        height=800,\n",
    "        yaxis={'autorange': 'reversed'}\n",
    "    )\n",
    "\n",
    "    # Mostra il plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "class_names = trainset.classes\n",
    "plot_confusion_matrix(confusion_matrix,class_names)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
